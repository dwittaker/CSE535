{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KERAS Test CSE535_Project_2 post submission.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dwittaker/CSE535/blob/master/KERAS_Test_CSE535_Project_2_post_submission.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Cygrd_dhTDl",
        "colab_type": "text"
      },
      "source": [
        "# Header"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_V0qK868hMpD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Fri Feb 14 08:38:55 2020\n",
        "\n",
        "This script makes use of JSON file based data (with posenet output) for the classification of sign language videos.\n",
        "It commences with pre-processing of data including extraction of the data into a useful format, \n",
        "followed by the development of features including transformation into various formats.\n",
        "Subsequently, the data is fed into various classification algorithms where we try to\n",
        "classify the sign language portrayed in the video.\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "from os import walk\n",
        "from os import path\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "import pywt\n",
        "from scipy.fftpack import fft\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import sklearn.preprocessing as skprep\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import pickle\n",
        "import keras\n",
        "import keras.utils\n",
        "from keras.optimizers import Adam\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, LSTM\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "pd.set_option('expand_frame_repr', True)\n",
        "path_testdata = '/content/drive/My Drive/poseclassifier/test'\n",
        "#path_trainingdata = '..\\\\..\\\\flaskr\\\\uploads' #original posenet upload directory. no longer used\n",
        "path_trainingdata2 = '/content/drive/My Drive/poseclassifier/training'\n",
        "path_tempfiles = '/content/drive/My Drive/CSE535Temp/'\n",
        "#path_classifydata = '..\\\\..\\\\flaskr\\\\uploads\\\\classify' #original posenet upload directory. no longer used\n",
        "\n",
        "#list_excludedparts = ['leftKnee', 'rightKnee', 'leftAnkle', 'rightAnkle']\n",
        "list_excludedparts = []\n",
        "listpartsinuse = ['nose', 'leftEye','rightEye','leftEar', 'rightEar',\n",
        "                      'leftShoulder','rightShoulder', 'leftHip', 'rightHip',\n",
        "                      'leftKnee', 'rightKnee', 'leftAnkle', 'rightAnkle',\n",
        "                      'leftElbow', 'rightElbow', 'leftWrist', 'rightWrist'\n",
        "                      ]             \n",
        "\n",
        "list_gesture = ['BUY', 'COMMUNICATE', 'FUN', 'HOPE', 'MOTHER', 'REALLY']\n",
        "#list_gesture = ['BUY', 'HOUSE', 'FUN', 'HOPE', 'ARRIVE', 'REALLY', 'READ', 'LIP', 'MOUTH', 'SOME', 'COMMUNICATE', 'WRITE', 'CREATE', 'PRETEND', 'SISTER', 'MAN', 'ONE', 'DRIVE', 'PERFECT', 'MOTHER']\n",
        "#list_gestureindex = [i.key for i in enumerate(list_gesture)]\n",
        "ftst = []\n",
        "ftrn = []\n",
        "ftrn2 = []\n",
        "dict_tst = {}\n",
        "dict_trn = {}\n",
        "dict_trn2 = {}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDfoHM-ZhqAL",
        "colab_type": "text"
      },
      "source": [
        "# Feature Development"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyXCc5BJhhM_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This just returns the plain data with no transformation\n",
        "def plainfunc(lst):\n",
        "    # print('plain')\n",
        "    return [lst, ['plot', '']]\n",
        "\n",
        "#This transforms the data using Fast Fourier Transform\n",
        "def fftfunc(lst):\n",
        "    fftresult = np.abs(fft(lst))\n",
        "    half = len(fftresult) // 2\n",
        "    return [fftresult[half:], ['stem', '']]\n",
        "\n",
        "#This transforms the data using basic mean (average) for each feature column\n",
        "def meanfunc(lst):\n",
        "    return [[sum(lst) / float(len(lst))], ['plot', 'o']]\n",
        "\n",
        "#This transforms the data using basic standard deviation for each feature column\n",
        "def stdfunc(lst):\n",
        "    return [[np.std(lst)], ['plot', 'o']]\n",
        "\n",
        "#This transforms the data using basic variance for each feature column\n",
        "def varfunc(lst):\n",
        "    return [[np.var(lst)], ['plot', 'o']]\n",
        "\n",
        "#This transforms the data using Discrete Wavelets for each feature column\n",
        "def dwtmultifunc(lst):\n",
        "    # print('dwting')\n",
        "    # w = pywt.Wavelet('db2')\n",
        "    # lst2 = map(int,lst)\n",
        "    #    cA, cD = pywt.dwt(lst, wavelet=w, mode='smooth')\n",
        "    w = pywt.Wavelet('db1')\n",
        "    cData = []\n",
        "    lvl = 4\n",
        "    if pywt.dwt_max_level(len(lst), w) < lvl:\n",
        "        lvl = pywt.dwt_max_level(len(lst), w)\n",
        "    cData = pywt.wavedec(lst, w, level=lvl)  # wavelet=w, mode='smooth')\n",
        "    return [cData[0], ['plot', '']]\n",
        "    # return dwt(lst)\n",
        "\n",
        "#This transforms the data using basic version of Discrete Wavelets for each feature column\n",
        "def dwtbasicfunc(lst):\n",
        "    w = pywt.Wavelet('db2')\n",
        "    cData = pywt.dwt(lst, w)  # wavelet=w, mode='smooth')\n",
        "    return [cData[0], ['plot', '']]\n",
        "\n",
        "#This transforms the data using basic root mean square for each feature column\n",
        "def rmsfunc(lst):\n",
        "    return [[np.sqrt(np.mean(np.square(lst)))], ['plot', 'o']]\n",
        "\n",
        "#This transforms the data using the maximum value for each feature column\n",
        "def maxfunc(lst):\n",
        "    return [[np.max(lst)], ['plot', 'o']]\n",
        "\n",
        "#This transforms the data using the minimum value for each feature column\n",
        "def minfunc(lst):\n",
        "    return [[np.min(lst)], ['plot', 'o']]\n",
        "\n",
        "#This transforms the data using the energy function for each feature column\n",
        "def energyfunc(lst):\n",
        "    e = sum(np.square(np.absolute(lst)))\n",
        "    return [[e], ['plot', 'o']]\n",
        "\n",
        "#This transforms the data using diffNormRawData for each feature column\n",
        "def dNRD(lst):\n",
        "    return zCRMaxDfunction(lst, 'DNRD')\n",
        "\n",
        "#This transforms the data using zeroCrossingArray for each feature column\n",
        "def zCR(lst):\n",
        "    return zCRMaxDfunction(lst, 'ZCR')\n",
        "\n",
        "#This transforms the data using maxDiffArray for each feature column\n",
        "def mDA(lst):\n",
        "    return zCRMaxDfunction(lst, 'MDA')\n",
        "\n",
        "#This transforms the data using DNRD, ZCR and MDA for each feature column\n",
        "def ZMD(lst):\n",
        "    return zCRMaxDfunction(lst, 'ALL')\n",
        "\n",
        "\n",
        "#This function handles the 3 transformations above\n",
        "def zCRMaxDfunction(lst, func):\n",
        "\n",
        "    diffNormRawData = np.diff(lst)\n",
        "    zeroCrossingArray = np.array([])\n",
        "    maxDiffArray = np.array([])\n",
        "\n",
        "    if diffNormRawData[0] > 0:\n",
        "        initSign = 1\n",
        "    else:\n",
        "        initSign = 0\n",
        "\n",
        "    windowSize = 5;\n",
        "\n",
        "    for x in range(1, len(diffNormRawData)):\n",
        "        if diffNormRawData[x] > 0:\n",
        "            newSign = 1\n",
        "        else:\n",
        "            newSign = 0\n",
        "\n",
        "        if initSign != newSign:\n",
        "            zeroCrossingArray = np.append(zeroCrossingArray, x)\n",
        "            initSign = newSign\n",
        "            maxIndex = np.minimum(len(diffNormRawData), x + windowSize)\n",
        "            minIndex = np.maximum(0, x - windowSize)\n",
        "\n",
        "            maxVal = np.amax(diffNormRawData[minIndex:maxIndex])\n",
        "            minVal = np.amin(diffNormRawData[minIndex:maxIndex])\n",
        "\n",
        "            maxDiffArray = np.append(maxDiffArray, (maxVal - minVal))\n",
        "\n",
        "    index = np.argsort(-maxDiffArray)\n",
        "\n",
        "    #Based on the parameter provided, return the appropriately transformed data\n",
        "    itm1 = list(diffNormRawData) if func in ['DNRD', 'ALL'] else []\n",
        "    itm2 = list(zeroCrossingArray[index[0:5]]) if len(zeroCrossingArray) > 0 else 0.0\n",
        "    itm2 = itm2 if func in ['ZCR', 'ALL'] else []\n",
        "    itm3 = list(maxDiffArray[index[0:5]]) if len(maxDiffArray) > 0 else 0.0\n",
        "    itm3 = itm3 if func in ['MDA', 'ALL'] else []\n",
        "    \n",
        "    return [[itm1, itm2, itm3], ['plot', 'o']]\n",
        "\n",
        "def clip(l, limit):\n",
        "    #Clips the sequence to the central limit values\n",
        "    lo = hi = (len(l) - limit) / 2\n",
        "    lo = math.ceil(lo)\n",
        "    hi = math.floor(hi)\n",
        "    return l[lo:-hi]\n",
        "\n",
        "def developfeaturematrix(features, dataset, aggregate, limit):\n",
        "    #names of available functions as listed above\n",
        "    ftroptions = {\n",
        "        'plain': plainfunc,\n",
        "        'energy': energyfunc,\n",
        "        'fft': fftfunc,\n",
        "        'mean': meanfunc,\n",
        "        'std': stdfunc,\n",
        "        'var': varfunc,\n",
        "        'dwt': dwtbasicfunc,\n",
        "        'dwtMulti': dwtmultifunc,\n",
        "        'rms': rmsfunc,\n",
        "        'max': maxfunc,\n",
        "        'min': minfunc,\n",
        "        'zcr': zCR,\n",
        "        'mda': mDA,\n",
        "        'dnrd': dNRD,\n",
        "        'zmd': ZMD\n",
        "        #'box': boxplotfunc,\n",
        "\n",
        "    }\n",
        "\n",
        "    listcolumns = [col for col in dataset.columns if col not in ['Gesture', 'Sample', 'Frame']]\n",
        "\n",
        "    testarr = []\n",
        "    listofrowstoadd = np.array([])\n",
        "    rowslist = []\n",
        "    \n",
        "    if not aggregate:\n",
        "        featmatrix = []\n",
        "        onerow = []\n",
        "        for i_gesture in range(len(list_gesture)):\n",
        "            print(\"Feature Creation for Gesture : \" + str(i_gesture))\n",
        "            totsam = len(dataset[(dataset.Gesture == i_gesture)].Sample.unique())\n",
        "            for x_sample in range(totsam):\n",
        "                onerow = [i_gesture, x_sample]\n",
        "\n",
        "                for col in listcolumns:\n",
        "                    lst = dataset.loc[(dataset.Gesture == i_gesture) & (dataset.Sample == x_sample), col].values\n",
        "                    lst = clip(lst, limit)\n",
        "                    for key, _ in ftroptions.items():\n",
        "                        if key in features:\n",
        "                            rslt = ftroptions[key](lst)\n",
        "                            rslt2 = flatten2list(rslt[0])\n",
        "                            onerow.extend(rslt2)\n",
        "                featmatrix.append(flatten2list(onerow))\n",
        "\n",
        "    else:\n",
        "        print(\"aggregate style\")\n",
        "        featmatrix = []\n",
        "        onerow = []\n",
        "        for i_gesture in range(len(list_gesture)):\n",
        "            print(\"Feature Creation for Gesture : \" + str(i_gesture))\n",
        "            totsam = len(dataset[(dataset.Gesture == i_gesture)].Sample.unique())\n",
        "            for x_sample in range(totsam):\n",
        "                setdata = dataset.loc[(dataset.Gesture == i_gesture) & (dataset.Sample == x_sample), listcolumns]\n",
        "                onerow = [i_gesture, x_sample]\n",
        "                frameend = np.max(setdata['Frame'])\n",
        "                onerow.extend(setdata.mean(axis=1, skipna=True))\n",
        "                featmatrix.append(flatten2list(onerow))\n",
        "\n",
        "\n",
        "    return np.array(featmatrix)\n",
        "\n",
        "#https://symbiosisacademy.org/tutorial-index/python-flatten-nested-lists-tuples-sets/\n",
        "def flatten2list(object):\n",
        "    gather = []\n",
        "    for item in object:\n",
        "        if isinstance(item, (list, tuple, set)):\n",
        "            gather.extend(flatten2list(item))\n",
        "        else:\n",
        "            gather.append(item)\n",
        "    return gather\n",
        "### +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "def runpca(matrix, perc):\n",
        "    #Use Principal Component Analysis to whittle down the available columns to \n",
        "    #determine the most likely useful columns. \n",
        "    classcol = np.array(list(zip(*matrix))[0])\n",
        "    # # Y_train = [list_gesture.index(i) for i in Y_train]\n",
        "    pcamatrix = np.array([item[1 - len(item):] for item in matrix])\n",
        "\n",
        "    X_std = skprep.StandardScaler().fit_transform(pcamatrix)\n",
        "\n",
        "    cov_mat = np.cov(X_std.T)\n",
        "\n",
        "    eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
        "\n",
        "    # Make a list of (eigenvalue, eigenvector) tuples\n",
        "    eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:, i]) for i in range(len(eig_vals))]\n",
        "\n",
        "    # Sort the (eigenvalue, eigenvector) tuples from high to low\n",
        "    eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "    exp_var_percentage = perc  # Threshold of explained variance\n",
        "    tot = sum(eig_vals)\n",
        "    var_exp = [(i / tot) * 100 for i in sorted(eig_vals, reverse=True)]\n",
        "    cum_var_exp = np.cumsum(var_exp)\n",
        "\n",
        "    num_vec_to_keep = 0\n",
        "\n",
        "    for index, percentage in enumerate(cum_var_exp):\n",
        "        if percentage > exp_var_percentage:\n",
        "            num_vec_to_keep = index + 0\n",
        "            break\n",
        "    print(\"number of vec to keep=%s\" % num_vec_to_keep)\n",
        "\n",
        "    # Compute the projection matrix based on the top eigen vectors\n",
        "    num_features = X_std.shape[1]\n",
        "    proj_mat = eig_pairs[0][1].reshape(num_features, 1)\n",
        "    for eig_vec_idx in range(1, num_vec_to_keep):\n",
        "        proj_mat = np.hstack((proj_mat, eig_pairs[eig_vec_idx][1].reshape(num_features, 1)))\n",
        "\n",
        "    # Project the data\n",
        "    pca_data = X_std.dot(proj_mat)\n",
        "    # newnumcol = pca_data.shape[1]\n",
        "    myclasscol = [[i] for i in classcol]\n",
        "    pca_data = np.append(pca_data, myclasscol, 1)\n",
        "\n",
        "    print(\"PCA Done - New Feature Matrix created\")\n",
        "    return pca_data\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiDwlogliWjo",
        "colab_type": "text"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFUhtBDUIfp9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def printcharts(dataset, activity):\n",
        "    #Used to visually review the data for possible common patterns amongst different feature sets\n",
        "    fontsize = 10\n",
        "    pltsize = 18\n",
        "    x = 0\n",
        "    \n",
        "    listcolumns = [col for col in dataset.columns if col not in ['Gesture', 'Sample', 'Frame']]\n",
        "    \n",
        "    list_gest = list_gesture#[:1]\n",
        "    fig, axes = plt.subplots(24, 2, figsize=(pltsize, 24*4*4*2))\n",
        "    rowcnt = 0\n",
        "    for i_gesture in range(len(list_gest)):\n",
        "            for i, col in enumerate(sorted(listcolumns)):\n",
        "                rowcnt = rowcnt + 1 if x == 1 else rowcnt\n",
        "                x = i % 2\n",
        "\n",
        "                for x_sample in range(len(dataset[(dataset.Gesture == i_gesture)].Sample.unique())):\n",
        "                    yvalues = dataset.loc[(dataset.Gesture == i_gesture) & (dataset.Sample == x_sample), col].values\n",
        "                    xvalues = [i for i in range(len(yvalues))]\n",
        "                    axes[rowcnt, x].set_title(\"Gesture\"+ str(i_gesture) + \" - \" + col, loc='left', fontsize=8, position=(0.0,0.1))\n",
        "                    \n",
        "                    axes[rowcnt, x].grid(True)\n",
        "                    axes[rowcnt, x].yaxis.set_ticks([])\n",
        "                    plt.yticks(xvalues, \"\")\n",
        "                    lines, = axes[rowcnt, x].plot(xvalues, yvalues)\n",
        "                    plt.setp(lines, linewidth=1.0)\n",
        "                \n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    fig.savefig(\"Chart\"+activity+\".png\")\n",
        "    print(listcolumns)\n",
        "\n",
        "def createtable(data):\n",
        "    dataset = []\n",
        "    i = 0\n",
        "    j = 0\n",
        "    for gesturekey, gesturevalues in data.items():\n",
        "        if 'PRACTICE' in gesturekey:\n",
        "            samplename = int(gesturekey.split('_')[2])\n",
        "            gesturename = gesturekey.split('_')[0] \n",
        "        else:\n",
        "            samplename = int(gesturekey.split('-')[1])\n",
        "            gesturename = gesturekey.split('-')[0]\n",
        "        for frame in gesturevalues:\n",
        "            keypoints = frame['keypoints']\n",
        "            j = 0\n",
        "            for point in keypoints:\n",
        "                if point['part'] not in list_excludedparts:\n",
        "                    dataset.append([list_gesture.index(gesturename), samplename, i, str('{:02d}'.format(j) +'-'+ point['part']), point['position']['x'], point['position']['y']])\n",
        "                    j += 1\n",
        "            i += 1\n",
        "        i = 0\n",
        "    #print(dataset.loc[(dataset.Gesture == 0)])\n",
        "    return pd.DataFrame(dataset, columns=['Gesture', 'Sample', 'Frame', 'Part', 'X', 'Y'])\n",
        "\n",
        "def fixcolumn(choice, txt):\n",
        "    #Basic data cleaning\n",
        "    if choice == \"partswide\":\n",
        "        return txt if '-' not in txt else txt.replace(\"(\",\"\").replace(\")\",\"\").replace(\"'\",\"\").split(', ')[1] + \"_\" + txt.replace(\"(\",\"\").replace(\")\",\"\").replace(\"'\",\"\").split(', ')[0]\n",
        "    else:\n",
        "        return txt if '-' not in txt else txt.replace(\"(\",\"\").replace(\")\",\"\").replace(\"'\",\"\").split(', ')[0] + \"_Frm_\" + str('{:03d}').format(int(txt.replace(\"(\",\"\").replace(\")\",\"\").replace(\"'\",\"\").split(', ')[1]))\n",
        "\n",
        "def sortcolumns(numbasic, lst):\n",
        "    basiclst = []\n",
        "    worklst = []\n",
        "    \n",
        "    for i in range(len(lst)):\n",
        "        if i <= (int(numbasic) - 1):\n",
        "            basiclst.append(lst[i])\n",
        "        else:\n",
        "            worklst.append(lst[i])\n",
        "    basiclst.extend(sorted(worklst))\n",
        "    #print(basiclst)\n",
        "    return basiclst\n",
        "\n",
        "def normalize_data(partswide_datatbl, activity):\n",
        "    #Used to normalize data based on the inherent mean, minimum and maximum values\n",
        "    global listpartsinuse\n",
        "    listcolumns = [col for col in partswide_datatbl.columns if col not in ['Part', 'Gesture', 'Sample', 'Frame']]\n",
        "    \n",
        "    print(\"Normalizing Data\")\n",
        "    print(partswide_datatbl.head)\n",
        "    #samplerangelimit = 4 if activity == \"TRAIN\" else 2\n",
        "    for i_gesture in range(len(list_gesture)):\n",
        "        for x_sample in range(len(partswide_datatbl[(partswide_datatbl.Gesture == i_gesture)].Sample.unique())):\n",
        "            for col in partswide_datatbl.columns:\n",
        "                if col in listcolumns:\n",
        "                    for part1 in listpartsinuse:\n",
        "                        try:\n",
        "                            workdata = partswide_datatbl.loc[(partswide_datatbl.Gesture == i_gesture) & (partswide_datatbl.Sample == x_sample) & (partswide_datatbl.Part == part1), col].values#.filter(items=[col])\n",
        "                            workdata2 = (workdata - np.mean(workdata))/((np.max(workdata-np.mean(workdata))-np.min(workdata-np.mean(workdata)))+0.0000000001)\n",
        "                            \n",
        "                            partswide_datatbl.loc[(partswide_datatbl.Gesture == i_gesture) & (partswide_datatbl.Sample == x_sample) & (partswide_datatbl.Part == part1), col] = workdata2\n",
        "                        except ValueError:  #raised if `y` is empty.\n",
        "                            #print(\"Value data: \" % (i_gesture, x_sample))#, part1 ))\n",
        "                            print(workdata)\n",
        "                            pass\n",
        "                        \n",
        "    return partswide_datatbl\n",
        "\n",
        "def distancecalc(X1, Y1, X2, Y2):\n",
        "    #Finding the distance between two objects' X and Y points\n",
        "    return  np.linalg.norm(np.array(X1, Y1)- np.array(X2, Y2))\n",
        "\n",
        "def originshiftcalc(dyn, stat1, stat2):\n",
        "    return (stat1.values[0] - dyn)/(stat1.values[0] - stat2.values[0])\n",
        "\n",
        "def normalize_to_nose(datatbl2, activity):\n",
        "\n",
        "    #Normalize the body part positions in relation to two main body parts that will not move\n",
        "    #as compared to hands. Uses the nose and the left hip as the anchor points\n",
        "\n",
        "    global listpartsinuse\n",
        "    \n",
        "    datatbl = np.array(datatbl2)\n",
        "    #samplerangelimit = 4 if activity == \"TRAIN\" else 2\n",
        "    for i_gesture in range(len(list_gesture)):\n",
        "        for x_sample in range(len(np.unique(datatbl[datatbl[:,0] == i_gesture][:,1]))):\n",
        "\n",
        "            print(i_gesture, x_sample)\n",
        "            setdata = datatbl[(datatbl[:,0] == i_gesture) & (datatbl[:,1] == x_sample)]\n",
        "            framestart = np.min(setdata[:,2])\n",
        "            frameend = np.max(setdata[:,2])\n",
        "            for i in range(framestart, frameend + 1):\n",
        "                workdata = setdata[setdata[:,2]== i]\n",
        "                nosedata = workdata[workdata[:,3] == '00-nose']\n",
        "                hipdata = workdata[workdata[:,3] == '11-leftHip']\n",
        "\n",
        "                updatedcol = (nosedata[0][4] - workdata[:,4])/(nosedata[0][4] - hipdata[0][4])\n",
        "                datatbl2.loc[(datatbl2.Gesture == i_gesture) & (datatbl2.Sample == x_sample) & (datatbl2.Frame == i), 'X'] = updatedcol\n",
        "\n",
        "                updatedcol = (nosedata[0][5] - workdata[:,5])/(nosedata[0][5] - hipdata[0][5])\n",
        "                datatbl2.loc[(datatbl2.Gesture == i_gesture) & (datatbl2.Sample == x_sample) & (datatbl2.Frame == i), 'Y'] = updatedcol\n",
        "\n",
        "    return datatbl\n",
        "\n",
        "def createtable_partswide(basicdata): \n",
        "\n",
        "    #Transforming the data from row based categories to spread out all body parts as independent feature columns\n",
        "    pivoteddataset = basicdata.pivot_table(['X','Y'], ['Gesture', 'Sample', 'Frame'], 'Part',fill_value=0)\n",
        "    flattened = pd.DataFrame(pivoteddataset.to_records())\n",
        "    flattened.columns = [fixcolumn('partswide', col) for col in flattened.columns]\n",
        "    flattened = flattened.reindex(sortcolumns(4, flattened.columns), axis=1)\n",
        "    \n",
        "    # normalized = normalize_data(flattened, \"TRAIN\")\n",
        "    # return normalized\n",
        "    return flattened\n",
        "\n",
        "def createlist_frameswide(maxframecnt, partswide):\n",
        "\n",
        "    #Transforming the partswide into a a list based format for all columns individually and pad as necessary\n",
        "    #to equalize the length of all features \n",
        "    listcolumns = [col for col in partswide.columns if col not in ['Gesture', 'Sample', 'Frame']]\n",
        "    testarr = []\n",
        "    listofrowstoadd = np.array([])\n",
        "    rowslist = []\n",
        "    for i_gesture in range(len(list_gesture)):\n",
        "        totsam = len(partswide[(partswide.Gesture == i_gesture)].Sample.unique())\n",
        "        for x_sample in range(totsam):\n",
        "            currlen = len(partswide[(partswide.Gesture == i_gesture) & (partswide.Sample == x_sample)])\n",
        "            padlength = maxframecnt - currlen\n",
        "\n",
        "            for i_pad in range(padlength):\n",
        "                testarr.append(np.array([i_gesture, x_sample, currlen + i_pad,  # np.zeros(34)\n",
        "                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "                                                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
        "                                                     ]))\n",
        "\n",
        "    print(\"Just before sort/add - time is {}\".format(datetime.datetime.now().strftime(\"%d-%b-%Y %H:%M:%S.%f\")))\n",
        "    if len(testarr) > 0:\n",
        "        partswide = partswide.append(pd.DataFrame(np.array(testarr), columns=partswide.columns), ignore_index=True)\n",
        "        partswide.sort_values(by=['Gesture', 'Sample', 'Frame'], inplace=True)\n",
        "        partswide = partswide.reset_index(drop=True)\n",
        "    print(\"Midway through frameswide. Pivoting time is {}\".format(datetime.datetime.now().strftime(\"%d-%b-%Y %H:%M:%S.%f\")))\n",
        "\n",
        "\n",
        "    pivoteddataset = partswide.pivot_table(listcolumns, ['Gesture', 'Sample'], 'Frame', fill_value=0)\n",
        "    flattened = pd.DataFrame(pivoteddataset.to_records())\n",
        "    flattened.columns = [fixcolumn('frameswide', col) for col in flattened.columns]\n",
        "    flattened = flattened.reindex(sortcolumns(3, flattened.columns), axis=1)\n",
        "\n",
        "    return flattened\n",
        "\n",
        "                \n",
        "def padlengths(featuredatasets):\n",
        "    #Further padding of the finalized dataset \n",
        "    maxlencnt = 0\n",
        "    new_featuredatasets = []#np.array()\n",
        "    \n",
        "    for arr in featuredatasets:\n",
        "        if len(arr) > 0:\n",
        "          length = max(map(len, arr))\n",
        "          if maxlencnt < length:\n",
        "              maxlencnt = length\n",
        "\n",
        "    for arr2 in featuredatasets:\n",
        "        new_featuredatasets.append(np.array([xi + [0] * (maxlencnt - len(xi)) for xi in arr2]))\n",
        "\n",
        "    print(\"Max Len count\" + str(maxlencnt))\n",
        "    return new_featuredatasets\n",
        "\n",
        "### +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "### +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "### +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "def checkdatafile(filename):\n",
        "    return path.exists(path_tempfiles+filename + '_export.csv')\n",
        "\n",
        "def savedatafile(pd, filename):\n",
        "    pd.to_csv(r''+path_tempfiles+filename+'_export.csv', index = True, header=True)\n",
        "\n",
        "def readdatafile(filename):\n",
        "    return pd.read_csv(r''+path_tempfiles+filename+'_export.csv', index_col = 0, header=0)\n",
        "\n",
        "def savenumpydata(npdata, filename):\n",
        "    np.savetxt(r''+path_tempfiles+filename+'_export.csv', npdata, delimiter = ',')\n",
        "\n",
        "def readnumpydata(filename):\n",
        "    return np.loadtxt(r''+path_tempfiles+filename+'_export.csv', delimiter=',')\n",
        "\n",
        "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "def createClassifierData(test_data, train_data):\n",
        "    #splitting out the rows of training and test data into X and Y values\n",
        "    #could also be done using sklearn\n",
        "    Y_train = np.array(list(zip(*train_data))[0])\n",
        "    \n",
        "    X_train = [item[1 - len(item):] for item in train_data]\n",
        "    \n",
        "    Y_test = np.array(list(zip(*test_data))[0]) \n",
        "    X_test = [item[1 - len(item):] for item in test_data] \n",
        "\n",
        "    #X_train, X_test, Y_train, Y_test = train_test_split(X_train, Y_train, test_size = 0.20, random_state = 42)\n",
        "\n",
        "    return X_test, Y_test, X_train, Y_train\n",
        "\n",
        "def preparedata(features, forcenewfile, forcenewfeatures, limit):\n",
        "    #This function extracts the data from the JSON files, does transforms \n",
        "    #into various formats and returns the data\n",
        "    print(\"Data Preparation starting time is {}\".format(datetime.datetime.now().strftime(\"%d-%b-%Y %H:%M:%S.%f\")))\n",
        "    versionnum = \"_vpost1\"\n",
        "    #Based on parameters, run through the directory to open the json files and setup the training and test datasets\n",
        "    if not (checkdatafile(\"test_original\"+versionnum) and forcenewfile == False):\n",
        "        for (dirpath, dirnames, filenames) in walk(path_testdata):\n",
        "            ftst.append([dirpath, dirnames, filenames])\n",
        "        \n",
        "        for (dirpath, dirnames, filenames) in walk(path_trainingdata2):\n",
        "            ftrn2.append([dirpath, dirnames, filenames])\n",
        "            \n",
        "        for i in range(len(ftst[0][1])):\n",
        "            if str(ftst[0][1][i]).split('-')[0] in list_gesture:\n",
        "                dict_tst[str(ftst[0][1][i]).split('-')[0] + \"-0\"] = json.load(open(ftst[i + 1][0]+'/key_points.json'))\n",
        "          \n",
        "        for i in range(len(ftrn2[0][1])):\n",
        "            if str(ftrn2[0][1][i].upper().split('_')[0]) in list_gesture: \n",
        "                for x in range(1, len(ftrn2)):\n",
        "                    if str(ftrn2[0][1][i].upper().split('_')[0]) in ftrn2[x][0].upper():\n",
        "                        for cnt in range(len(ftrn2[x][2])):\n",
        "                            dict_trn2[str(ftrn2[0][1][i].upper().split('_')[0])+\"-\" + str(cnt)] = json.load(open(ftrn2[x][0]+ \"/\" + ftrn2[x][2][cnt]))\n",
        "\n",
        "\n",
        "   \n",
        "    #Based on the parameters, check if we are to recreate or re-use existing data\n",
        "    #Next few blocks check parameters and then experiments are done with various normalize functions\n",
        "    #Normalize to anchor points or normalize based on mean\n",
        "    print(\"Data Normalization start time is {}\".format(datetime.datetime.now().strftime(\"%d-%b-%Y %H:%M:%S.%f\")))\n",
        "    #------------------------------------------\n",
        "    if checkdatafile(\"test_normalized\"+versionnum) and forcenewfile == False:\n",
        "        pd_tst = readdatafile(\"test_normalized\"+versionnum)\n",
        "    else:\n",
        "        pd_tst1 = createtable(dict_tst)\n",
        "        #savedatafile(pd_tst1, \"test_original\" + versionnum)\n",
        "        #pd_tst = pd_tst1\n",
        "        #pd_tst = normalize_to_nose(pd_tst1,'TEST')\n",
        "        pd_tst = normalize_data(pd_tst1, \"TEST\")\n",
        "        savedatafile(pd_tst, \"test_normalized\"+versionnum)\n",
        "    #------------------------------------------\n",
        "    if checkdatafile(\"train_normalized\"+versionnum) and forcenewfile == False:\n",
        "        pd_trn = readdatafile(\"train_normalized\"+versionnum)\n",
        "    else:\n",
        "        pd_trn2 = createtable(dict_trn2)\n",
        "        #savedatafile(pd_trn2, \"train_original\" + versionnum)\n",
        "        #pd_trn = pd_trn2\n",
        "        #pd_trn = normalize_to_nose(pd_trn2, 'TRAIN')\n",
        "        pd_trn = normalize_data(pd_trn2, \"TRAIN\")\n",
        "        savedatafile(pd_trn, \"train_normalized\"+versionnum)\n",
        "    #------------------------------------------\n",
        "    #pd_trn2 = createtable(dict_trn2)\n",
        "    #pd_trn2 = normalize_data(pd_trn2, \"TRAIN\")\n",
        "    #pd_trn = normalize_to_nose(pd_trn, 'TRAIN')\n",
        "    #------------------------------------------\n",
        "    #print(pd_trn.head(100))\n",
        "    \n",
        "    ##################\n",
        "    #Re-shape the data as necessary to formulate featuresets\n",
        "    print(\"Data Re-Shaping start time is {}\".format(datetime.datetime.now().strftime(\"%d-%b-%Y %H:%M:%S.%f\"))) \n",
        "    if checkdatafile(\"test_partswide\"+versionnum) and forcenewfile == False:\n",
        "        pd_tst_parts = readdatafile(\"test_partswide\"+versionnum)\n",
        "    else:\n",
        "        pd_tst_parts = createtable_partswide(pd_tst)\n",
        "        savedatafile(pd_tst_parts, \"test_partswide\"+versionnum)\n",
        "\n",
        "    if checkdatafile(\"train_partswide\"+versionnum) and forcenewfile == False:\n",
        "        pd_trn_parts = readdatafile(\"train_partswide\"+versionnum)\n",
        "    else:\n",
        "        pd_trn_parts = createtable_partswide(pd_trn)\n",
        "        savedatafile(pd_trn_parts, \"train_partswide\"+versionnum)\n",
        "\n",
        "    #Basic review of the data visually to understand and determine existence of patterns\n",
        "    #print(\"Printing charts at time - {}\".format(datetime.datetime.now().strftime(\"%d-%b-%Y %H:%M:%S.%f\")))\n",
        "    #printcharts(pd_tst_parts, \"test\")\n",
        "    #printcharts(pd_trn_parts, \"train\")\n",
        "\n",
        "    ##################\n",
        "    #Develop featuresets based on function and other parameters chosen\n",
        "    if checkdatafile(\"train_features\"+versionnum) and forcenewfeatures == False:\n",
        "        np_tst_parts_features = readnumpydata(\"test_features\"+versionnum)\n",
        "        np_trn_parts_features = readnumpydata(\"train_features\"+versionnum)\n",
        "        print(\"Pulled Re-shaped Data ending time at {}\".format(datetime.datetime.now().strftime(\"%d-%b-%Y %H:%M:%S.%f\")))\n",
        "    else:\n",
        "        print(\"Data 1 Re-shaping Test Frames Wide starting time is {}\".format(\n",
        "            datetime.datetime.now().strftime(\"%d-%b-%Y %H:%M:%S.%f\")))\n",
        "        np_tst_parts_features = developfeaturematrix(features, pd_tst_parts, False, limit)\n",
        "        print(\"Data 2 Re-shaping Train Frames Wide starting time is {}\".format(\n",
        "            datetime.datetime.now().strftime(\"%d-%b-%Y %H:%M:%S.%f\")))\n",
        "        np_trn_parts_features = developfeaturematrix(features, pd_trn_parts, False, limit)\n",
        "\n",
        "        #Further Padding of overall datasets to ensure consistent lengths for ML Classifiers\n",
        "        print(\"Padding Lengths starting time is {}\".format(datetime.datetime.now().strftime(\"%d-%b-%Y %H:%M:%S.%f\")))\n",
        "        np_tst_parts_features, np_trn_parts_features = padlengths([np_tst_parts_features, np_trn_parts_features])\n",
        "        savenumpydata(np_tst_parts_features, \"test_features\"+versionnum)\n",
        "        savenumpydata(np_trn_parts_features, \"train_features\"+versionnum)\n",
        "\n",
        "    #PCA disabled for the moment - 2020/03/01\n",
        "    #print(\"RUN PCA starting time is {}\".format(datetime.datetime.now().strftime(\"%d-%b-%Y %H:%M:%S.%f\")))\n",
        "    #np_tst_parts_features = runpca(np_tst_parts_features, 60)\n",
        "    #np_trn_parts_features = runpca(np_trn_parts_features, 60)\n",
        "    \n",
        "\n",
        "    return np_tst_parts_features, np_trn_parts_features\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dz4hThnchHVw",
        "colab_type": "text"
      },
      "source": [
        "# Classifiers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGooXw44g1Y0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def classify_ann(X_test, Y_test, X_train, Y_train):\n",
        "    #Using an Artificial Neural Network to classify the data using a \n",
        "    #Grid Search to try and find the best hyperparameters\n",
        "\n",
        "    \n",
        "    #https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
        "    num_hidden = int(len(X_test[0])/2)\n",
        "\n",
        "    parameter_space = {\n",
        "    'hidden_layer_sizes': [(num_hidden, round(num_hidden/2), round(num_hidden/4)), (num_hidden,num_hidden*2,num_hidden), (num_hidden,)],\n",
        "    'activation': ['tanh', 'relu', 'logistic'],\n",
        "    'solver': ['sgd', 'adam'],\n",
        "    #'alpha': [0.0001, 0.05],\n",
        "    'learning_rate': ['constant','adaptive'],\n",
        "    'learning_rate_init': [0.0001, 0.0005,0.001]\n",
        "    }\n",
        "\n",
        "    #from sklearn.preprocessing import StandardScaler\n",
        "    #scaler = StandardScaler()\n",
        "    #scaler.fit(X_train)\n",
        "    \n",
        "    #X_train = scaler.transform(X_train)\n",
        "    #X_test = scaler.transform(X_test)\n",
        "    from sklearn.neural_network import MLPClassifier  \n",
        "    mlp = MLPClassifier(max_iter=800, verbose=True, tol=0.000000100, momentum=0.9,early_stopping=False)\n",
        "    #37-44% range\n",
        "    # mlp = MLPClassifier(hidden_layer_sizes=(num_hidden, round(num_hidden/2), round(num_hidden/4)), max_iter=800, activation='logistic', momentum=0.9,early_stopping=False,\n",
        "    #                     learning_rate='adaptive', verbose=True, tol=0.000000100, learning_rate_init=0.0001)\n",
        "\n",
        "    clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=2, verbose=1, )\n",
        "    clf.fit(X_train, Y_train)\n",
        "\n",
        "    # Best parameters set\n",
        "    print('Best parameters found:\\n', clf.best_params_)\n",
        "\n",
        "    # All results\n",
        "    means = clf.cv_results_['mean_test_score']\n",
        "    stds = clf.cv_results_['std_test_score']\n",
        "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
        "        print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
        "\n",
        "    predictions = clf.predict(X_test)\n",
        "    pickle.dump(mlp, open(\"model_ANN.pkl\", 'wb'))\n",
        "    print(classification_report(Y_test,predictions, target_names=list_gesture))\n",
        "    acc = accuracy_score(Y_test, predictions)\n",
        "    print(\"Accuracy: \"+str(acc))\n",
        "    rslt2 = precision_recall_fscore_support(Y_test,predictions)\n",
        "    \n",
        "def classify_NB(X_test, Y_test, X_train, Y_train):\n",
        "    #Using a Naive Bayes classifier to identify the action\n",
        "\n",
        "    from sklearn.naive_bayes import GaussianNB\n",
        "    gnb = GaussianNB().fit(X_train, Y_train)\n",
        "    gnb_predictions = gnb.predict(X_test)\n",
        "    pickle.dump(gnb_predictions, open(\"model_NB.pkl\", 'wb'))\n",
        "    accuracy = gnb.score(X_test, Y_test)\n",
        "\n",
        "    print(classification_report(Y_test, gnb_predictions, target_names=list_gesture))\n",
        "    acc = accuracy_score(Y_test, gnb_predictions)\n",
        "    print(\"Accuracy: \"+str(acc))\n",
        "    rslt2 = precision_recall_fscore_support(Y_test,gnb_predictions)\n",
        "\n",
        "\n",
        "def classify_KNN(X_test, Y_test, X_train, Y_train):\n",
        "    #Using a K Nearest Neighbor classifier to identify the action\n",
        "    from sklearn import neighbors\n",
        "    clf = neighbors.KNeighborsClassifier(n_neighbors=2)\n",
        "    clf.fit(X_train, Y_train)\n",
        "    knn_predictions = clf.predict(X_test)\n",
        "    pickle.dump(knn_predictions, open(\"model_KNN.pkl\", 'wb'))\n",
        "\n",
        "    print(classification_report(Y_test, knn_predictions, target_names=list_gesture))\n",
        "    acc = accuracy_score(Y_test, knn_predictions)\n",
        "    print(\"Accuracy: \"+str(acc))\n",
        "    rslt2 = precision_recall_fscore_support(Y_test,knn_predictions)\n",
        "\n",
        "def classify_LSTMCNN(X_test, Y_test, X_train, Y_train):\n",
        "    #Using Keras for a Deep Learning NN to identify the action\n",
        "    #Was modded from a CNN since we are using the posenet body position results\n",
        "    #LSTM is still being experimented with as a means to memorize the \n",
        "    #sequence without losing data to vanishing gradient\n",
        "    #IT IS MESSY\n",
        "    from sklearn.metrics import accuracy_score\n",
        "    from sklearn.metrics import precision_score\n",
        "    from sklearn.metrics import recall_score\n",
        "    from sklearn.metrics import f1_score\n",
        "    # fix random seed for reproducibility\n",
        "    np.random.seed(7)\n",
        "    \n",
        "    max_seq_length = 900\n",
        "    X_train = sequence.pad_sequences(X_train, maxlen=max_seq_length)\n",
        "    X_test = sequence.pad_sequences(X_test, maxlen=max_seq_length)\n",
        "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)  \n",
        "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "    print(X_train.shape)\n",
        "    print(X_test.shape)\n",
        "    print(Y_train.shape)\n",
        "    print(Y_test.shape)\n",
        "\n",
        "    #print(Y_train)\n",
        "    #print(Y_test)\n",
        "    orig_Y_train = Y_train\n",
        "    orig_Y_test = Y_test\n",
        "\n",
        "    Y_train = keras.utils.to_categorical(Y_train, num_classes=6)\n",
        "    Y_test = keras.utils.to_categorical(Y_test, num_classes=6)\n",
        "    model = Sequential()\n",
        "    \n",
        "    #model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "    #model.add(MaxPooling1D(pool_size=2))\n",
        "    \n",
        "    model.add(Flatten())\n",
        "\n",
        "    #model.add(LSTM(128, #return_sequences=True,\n",
        "    #            input_shape=(max_seq_length, 1)))  # returns a sequence of vectors of dimension 32\n",
        "    # #model.add(LSTM(32, return_sequences=True))  # returns a sequence of vectors of dimension 32\n",
        "    # #model.add(LSTM(32))  # return a single vector of dimension 32\n",
        "    # #model.add(Dropout(0.2))\n",
        "    model.add(LSTM(900))\n",
        "    model.add(Dense(900, activation='relu'))\n",
        "    model.add(Dense(450, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(200, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(100, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(6, activation='softmax'))\n",
        "    \n",
        "    # ####model.add(Flatten())\n",
        "    # model.add(Dense(100, activation='relu'))\n",
        "    # model.add(Dropout(0.2))\n",
        "    # model.add(Dense(32, activation='relu'))\n",
        "    # model.add(Dropout(0.2))\n",
        "    # model.add(Dense(6, activation='softmax'))\n",
        "    #model.add(Activation(\"softmax\"))\n",
        "\n",
        "    optim = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'])\n",
        "    model.build(X_train.shape)\n",
        "    print(model.summary())\n",
        "    print(model.optimizer.get_config())\n",
        "\n",
        "    \n",
        "    history = model.fit(X_train, Y_train, epochs=300, batch_size=415, verbose=0)\n",
        "    yhat_probs = model.predict(X_test, verbose=0)\n",
        "    yhat_classes = model.predict_classes(X_test, verbose=0)\n",
        "\n",
        "    print(orig_Y_test)\n",
        "    print(yhat_classes)\n",
        "    accuracy = accuracy_score(orig_Y_test, yhat_classes )\n",
        "    print('Accuracy: %f' % accuracy)\n",
        "    # precision tp / (tp + fp)\n",
        "    precision = precision_score(orig_Y_test, yhat_classes, average='micro' )\n",
        "    print('Precision: %f' % precision)\n",
        "    # recall: tp / (tp + fn)\n",
        "    recall = recall_score(orig_Y_test, yhat_classes, average='micro' )\n",
        "    print('Recall: %f' % recall)\n",
        "    # f1: 2 tp / (2 tp + fp + fn)\n",
        "    f1 = f1_score(orig_Y_test, yhat_classes, average='micro' )\n",
        "    print('F1 score: %f' % f1)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgD64K0JhEBx",
        "colab_type": "text"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLAjCcj2gvy7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##############################################################\n",
        "#### Several feature sets are being experimented with, to find the best \n",
        "#### combination for prediction accuracy etc\n",
        "\n",
        "#def main():\n",
        "\n",
        "#Master List\n",
        "#features = ['plain', 'fft','mean','std','var','dwt','dwtMulti','rms','max','min','energy', 'zcr', 'mda', \n",
        "#'dnrd', 'zmd']\n",
        "#features = ['plain','zCRMD','fft','dwt','mean','std','var','dwtMulti','rms','max','min','energy'] #36%\n",
        "#features = [ 'plain', 'fft', 'dwt'] 37%\n",
        "#features = [ 'fft', 'dwt', ] 45%\n",
        "\n",
        "#features = [ 'fft', 'dwt']\n",
        "#features = [ 'zcr','mda', 'energy'] -61 on knn\n",
        "#features = ['fft','dwt']\n",
        "features = ['plain']\n",
        "\n",
        "#The Data is extracted and prepared here\n",
        "dataarrtst, dataarrtrn = preparedata(features, forcenewfile = True, forcenewfeatures = True, limit = 30)\n",
        "#The data is simply split into X and Y sets for both the test and training datasets\n",
        "X_test, Y_test, X_train, Y_train = createClassifierData(dataarrtst, dataarrtrn)\n",
        "\n",
        "#Various classification algorithms are used here to make predictions for the actions\n",
        "#We basically compare their results to determine the best model\n",
        "#LSTM Network is still being experimented with\n",
        "#print(\"Classification - ANN starting time is {}\".format(datetime.datetime.now().strftime(\"%d-%b-%Y %H:%M:%S.%f\")))\n",
        "#classify_ann(X_test, Y_test, X_train, Y_train)\n",
        "# print(\"Classification - NB starting time is {}\".format(datetime.datetime.now().strftime(\"%d-%b-%Y %H:%M:%S.%f\")))\n",
        "# classify_NB(X_test, Y_test, X_train, Y_train)\n",
        "# print(\"Classification - KNN starting time is {}\".format(datetime.datetime.now().strftime(\"%d-%b-%Y %H:%M:%S.%f\")))\n",
        "# classify_KNN(X_test, Y_test, X_train, Y_train)\n",
        "print(\"Classification - LSTMCNN starting time is {}\".format(datetime.datetime.now().strftime(\"%d-%b-%Y %H:%M:%S.%f\")))\n",
        "classify_LSTMCNN(X_test, Y_test, X_train, Y_train)\n",
        "print(\"Finished\")\n",
        "\n",
        "# if __name__== \"__main__\":\n",
        "#    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jXpWYNibY5U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Mounting Drive location that will allow loading of JSON files and\n",
        "#read/write for Transformed Model Data and Model Parameters\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fz1kmCDsh0_x",
        "colab_type": "text"
      },
      "source": [
        "# Bottom"
      ]
    }
  ]
}